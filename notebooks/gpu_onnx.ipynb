{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "697717d2",
   "metadata": {},
   "source": [
    "# ONNX - DirectML GPU Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acdb44cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from optimum.exporters.onnx import main_export\n",
    "import onnx\n",
    "from pathlib import Path\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c505df6",
   "metadata": {},
   "source": [
    "## Verify connection to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b297a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "def available_gpu_onnx():\n",
    "\n",
    "    print(\"Available Providers:\", ort.get_available_providers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e073e985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Providers: ['DmlExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "available_gpu_onnx()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb235e0",
   "metadata": {},
   "source": [
    "## Inference with ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e80b8f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:55: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:55: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\joel\\AppData\\Local\\Temp\\ipykernel_560\\284463954.py:55: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  result = run_inference_dml(\"onnx_model\\model.onnx\", \"I really enjoyed this movie!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available providers: ['DmlExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joel\\AppData\\Local\\Temp\\ipykernel_560\\284463954.py:55: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  result = run_inference_dml(\"onnx_model\\model.onnx\", \"I really enjoyed this movie!\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Required inputs (['attention_mask', 'token_type_ids']) are missing from input feed (['input_ids']).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     49\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mraw_output\u001b[39m\u001b[33m\"\u001b[39m: outputs,\n\u001b[32m     50\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m     51\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprobabilities\u001b[39m\u001b[33m\"\u001b[39m: probabilities\n\u001b[32m     52\u001b[39m     }\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m result = \u001b[43mrun_inference_dml\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43monnx_model\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mmodel.onnx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mI really enjoyed this movie!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProbabilities: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mprobabilities\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mrun_inference_dml\u001b[39m\u001b[34m(onnx_model_path, text_input)\u001b[39m\n\u001b[32m     38\u001b[39m tokens = tokenizer(text_input, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mnp\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m outputs = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Process output based on your model type\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# This example assumes classification output\u001b[39;00m\n\u001b[32m     45\u001b[39m probabilities = outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joel\\Desktop\\AI-ML-Engineering-MonoRepo\\.venv\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:266\u001b[39m, in \u001b[36mSession.run\u001b[39m\u001b[34m(self, output_names, input_feed, run_options)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, output_names, input_feed, run_options=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    253\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[33;03m    Compute the predictions.\u001b[39;00m\n\u001b[32m    255\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m \u001b[33;03m        sess.run([output_name], {input_name: x})\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_names:\n\u001b[32m    268\u001b[39m         output_names = [output.name \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._outputs_meta]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joel\\Desktop\\AI-ML-Engineering-MonoRepo\\.venv\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:248\u001b[39m, in \u001b[36mSession._validate_input\u001b[39m\u001b[34m(self, feed_input_names)\u001b[39m\n\u001b[32m    246\u001b[39m         missing_input_names.append(\u001b[38;5;28minput\u001b[39m.name)\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing_input_names:\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    249\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRequired inputs (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_input_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) are missing from input feed (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeed_input_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    250\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Required inputs (['attention_mask', 'token_type_ids']) are missing from input feed (['input_ids'])."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def run_inference_dml(onnx_model_path, text_input):\n",
    "    \"\"\"\n",
    "    Run inference on an ONNX model using DirectML on AMD GPU.\n",
    "    \n",
    "    Args:\n",
    "        onnx_model_path: Path to the ONNX model\n",
    "        text_input: Text input for inference\n",
    "    \"\"\"\n",
    "    # Check available providers\n",
    "    providers = ort.get_available_providers()\n",
    "    print(f\"Available providers: {providers}\")\n",
    "    \n",
    "    # Ensure DirectML is available\n",
    "    if 'DmlExecutionProvider' not in providers:\n",
    "        raise RuntimeError(\"DirectML provider not found. Please ensure onnxruntime-directml is installed.\")\n",
    "    \n",
    "    # Create inference session with DirectML provider\n",
    "    session_options = ort.SessionOptions()\n",
    "    session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    session = ort.InferenceSession(\n",
    "        onnx_model_path,\n",
    "        sess_options=session_options,\n",
    "        providers=['DmlExecutionProvider']\n",
    "    )\n",
    "    \n",
    "    # Get the model's input name\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    \n",
    "    # Load tokenizer (should match the model you converted)\n",
    "    model_name = \"distilbert-base-uncased\"  # Replace with your model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenize input\n",
    "    tokens = tokenizer(text_input, return_tensors=\"np\")\n",
    "    \n",
    "    # Run inference\n",
    "    outputs = session.run(None, {input_name: tokens['input_ids']})\n",
    "    \n",
    "    # Process output based on your model type\n",
    "    # This example assumes classification output\n",
    "    probabilities = outputs[0]\n",
    "    prediction = np.argmax(probabilities, axis=1)\n",
    "    \n",
    "    return {\n",
    "        \"raw_output\": outputs,\n",
    "        \"prediction\": prediction,\n",
    "        \"probabilities\": probabilities\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "result = run_inference_dml(\"onnx_model\\model.onnx\", \"I really enjoyed this movie!\")\n",
    "print(f\"Prediction: {result['prediction']}\")\n",
    "print(f\"Probabilities: {result['probabilities']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
